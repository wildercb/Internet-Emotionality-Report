import requests
import datetime
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep, time
from random import randint
from textblob import TextBlob

def scrape_from_wayback(main_url, from_date, to_date, output_file, elements, max_reqs):
    titles, dates, polarities, subjectivities, day_of_week = [], [], [], [], []
    banned_days, processed_dates = set(), set()

    def get_day_of_week(date_str):
        year, month, day = int(date_str[:4]), int(date_str[4:6]), int(date_str[6:8])
        return datetime.date(year, month, day).strftime("%A")

    def is_banned(day):
        return day in banned_days

    start_time = time()
    reqs = 0
    min_page_char = 20000

    # Construct Wayback Machine URL for fetching archived pages
    url = f'https://web.archive.org/cdx/search/cdx?url={main_url}&from={from_date}&to={to_date}&output=json'
    response = requests.get(url)
    parse_url = response.json()

    for item in parse_url[1:]:
        if reqs >= max_reqs:
            break

        timestamp, original_url = item[1], item[2]
        current_date = timestamp[:8]

        if current_date in processed_dates or is_banned(current_date):
            continue

        wayback_url = f'https://web.archive.org/web/{timestamp}/{original_url}'
        try:
            page = requests.get(wayback_url).text
            if len(page) < min_page_char:
                banned_days.add(current_date)
                continue

            soup = BeautifulSoup(page, 'html.parser')
            for element in elements:
                for tag in soup.find_all(element.get('tag'), class_=element.get('class')):
                    title = tag.text.strip()
                    titles.append(title)
                    dates.append(current_date)
                    analysis = TextBlob(title)
                    polarities.append(analysis.sentiment.polarity)
                    subjectivities.append(analysis.sentiment.subjectivity)
                    day_of_week.append(get_day_of_week(current_date))

            processed_dates.add(current_date)
            reqs += 1
            sleep(randint(10, 20))
        except requests.exceptions.RequestException as e:
            print(f'Request failed: {e}')

    missed_days = pd.DataFrame({'MissedDays': list(banned_days)})
    missed_days.to_csv(f'{from_date}_{to_date}_missed.csv', index=False)

    data = pd.DataFrame({'Title': titles, 'Date': dates, 'WeekDay': day_of_week, 'Polarity': polarities, 'Subjectivity': subjectivities})
    data.to_csv(f'{output_file}_{from_date}_{current_date}.csv', index=False)

    return current_date

# Example usage
elements = [{'tag': 'p', 'class': 'title'}, {'tag': 'h3', 'class': None}, {'tag': 'h2', 'class': None}]
scrape_from_wayback('reddit.com', 20161122, 20161124, 'titles', elements, 20)
